{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wIe_NNFTYs_"
   },
   "source": [
    "# Data Mining Course Spark Exercise\n",
    "## Sharif University of Technology\n",
    "\n",
    "In this notebook we are going to analyze farsi wikipedia. \n",
    "Outline of the exercise:\n",
    "* Dataset preparation\n",
    "* Preprocessing (25 Points) \n",
    "* Exploration (20 Points) \n",
    "* TF-IDF + Search (55 Points)\n",
    "\n",
    "You should replace the `TODO` parts with your implementation. Remeber that each `TODO` may take multiple lines and you shouldn't limit your self to one-line codes.\n",
    "\n",
    "## Prerequisites\n",
    "You should be faimilar with [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). In this notebook you should use the following formula for tf-idf:\n",
    "$$f_{t,d}/len(d) \\times log(1 + \\frac{N}{n_t})$$\n",
    "\n",
    "## Warning: RDD api only\n",
    "You **can not** use Dataframe, Dataset, mllib, ml, ... apis of spark in this exercise. You should only use the [RDD api](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPC8u67dTYtA"
   },
   "source": [
    "# Section 0: Please enter your name below\n",
    "# Name:Sajjad Hashembeiki\n",
    "# Student Number:98107077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjLWtOdITYtB"
   },
   "source": [
    "# Section 1: Dataset preparation\n",
    "\n",
    "This section of notebook contains only shell commands. You don't need to completely understand each command or change anything.\n",
    "\n",
    "Please run all the paragraphs sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ezi5dS6YTYtC",
    "outputId": "1a1c063f-173b-42aa-9ba3-d89d428467d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -y install wget git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBbtThFKTYtD"
   },
   "source": [
    "## Download the dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W08JzklTYtE",
    "outputId": "873fcfaf-e6c4-4596-d38a-ac837dde4bdd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! wget --no-check-certificate -P . https://dumps.wikimedia.org/fawiki/latest/fawiki-latest-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP6Q0z6uTYtE"
   },
   "source": [
    "## Extract the dump (this may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUlUk28VTYtF"
   },
   "outputs": [],
   "source": [
    "! bzip2 -d fawiki-latest-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4DCV5w5TYtF"
   },
   "source": [
    "## Clone git project for converting wikipedia xml dump to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fl4Fm5dTYtF"
   },
   "outputs": [],
   "source": [
    "! pip install wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0jlwlM9TYtG"
   },
   "source": [
    "## Run the script to convert xml to json (this might take around 30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PspJR_ATYtG"
   },
   "outputs": [],
   "source": [
    "! python -m wikiextractor.WikiExtractor --json fawiki-latest-pages-articles-multistream.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NkdMpq4TYtH"
   },
   "source": [
    "## Ensure output files exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONN2PqvlTYtH"
   },
   "outputs": [],
   "source": [
    "! ls text/*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCPaEaL7TYtH"
   },
   "source": [
    "## Install Pypark & Initialization\n",
    "Uncomment this section if you use google colab or local pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isNdHD1HTYtH"
   },
   "outputs": [],
   "source": [
    "#! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WQVNsWITYtI"
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"MDA_2021\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywInWa6mTYtI"
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-4vcBCBTYtI"
   },
   "outputs": [],
   "source": [
    "articles_rdd = sc.textFile(\"text/*/*\") # Now you have a RDD with wikipedia posts\n",
    "articles_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jcfYLmOTYtJ"
   },
   "source": [
    "# Section 2: Preprocessing (25 Points)\n",
    "\n",
    "In this section we will remove useless part (for example /n and /u200c and ...) also find and remove stop words and remove the words with a low count in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be6tvT5RTYtJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "articles_rdd = #TODO: parse the json string\n",
    "cleansed_articles_rdd= #TODO: cleansed text\n",
    "words_rdd = #TODO: extract words from title and description\n",
    "words_count_rdd = #TODO: make an rdd with the count of each word\n",
    "top_100 = #TODO: find the 100 most common words\n",
    "stopwords = ['و', 'با'] #TODO: complete the list of stopwords based on top 100 common words\n",
    "articles_without_stopwords_rdd = #TODO: remove stopwords from the article title and text\n",
    "MIN_COUNT = 20\n",
    "uncommon_words = #TODO: list of the words that have occured less than MIN_COUNT in the whole corpus\n",
    "articles_cleaned_rdd = #TODO: remove uncommon words from articles_without_stopwords_rdd\n",
    "articles_cleaned_rdd.take(1) # This should output a dictionary with url,title and text keys. title and text should not have stopwords or uncommon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79dgk7QHTYtK"
   },
   "source": [
    "# Section 3: Exploration (20 Points)\n",
    "Please answer the following questions regarding the dataset:\n",
    "* How many unique 3-Letter words remain after the cleaning procedure?\n",
    "* What are the top 20 most common English trigrams in the corpus?\n",
    "* Plot a distribution from document lengths using appropriate bin sizes with 100 bins\n",
    "* What are the titles and urls of the 5 longest articles? \n",
    "* How many and what percentage of articles contain these words? [History, Politics, Medicine, Law, Economics, Engineering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS2K-g_4TYtK"
   },
   "source": [
    "# Section 4: TF-IDF + Searching (55 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsJEBmbrTYtK"
   },
   "outputs": [],
   "source": [
    "word_df_rdd = #TODO: calculate document frequncy for each word\n",
    "articles_tf_idf_vectors = #TODO: add `vector` key to articles_cleaned_rdd dictionary with the tf_idf dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZn3_bnLTYtL"
   },
   "source": [
    "## Searching\n",
    "In this section you should find articles that are about the topics mentioned in the last part of the third section.\n",
    "Report the percentage of articles that are about the mentioned topics, for example, report what percentage of the articles were about history?\n",
    "For each topic, report two of the most relevant articles along with the title and the url.\n",
    "Check this part in **theory** before practical implementation (Hint: Use the concept of tf-idf).\n",
    "All innovative solutions are also welcome. Compare the obtained results with the third part."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "name": "SUT - Data Mining Spark Exercise",
  "notebookId": 3958686303135329,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
